{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 3.\n",
    "Napisz własny system do indeksowania stron internetowych, \n",
    "który: <br/>\n",
    "- przegląda strony i zapamiętuje liczbę wystąpień poszczególnych słów na poszczególnych stronach\n",
    "- zachowuje się podobnie jak pythonowy słownik, gdzie kluczem jest słowo, a wartością lista stron na których to słowo występuje (bądź lista pusta).\n",
    "Strony powinny być uszeregowane malejąco względem podanej liczby wystąpień. <br/>\n",
    "Możesz też zaproponować własną strategię rankowania stron. <br/>\n",
    "Zakładamy, że indeksujemy tylko stronę wskazaną jako parametr odpowiedniej <br/>\n",
    "funkcji czy metody, oraz strony do których da się dojść po linkach i href w nie <br/>\n",
    "więcej niz z góry zadana liczba kroków. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "from collections import OrderedDict \n",
    "from operator import itemgetter\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "from collections import deque\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = {}\n",
    "visited = set()\n",
    "\n",
    "def make_soup(url):\n",
    "    try:\n",
    "        page = urllib.request.urlopen(url)\n",
    "    except urllib.request.HTTPError as e:\n",
    "        print('Ignored: ', e)\n",
    "        return -1\n",
    "        \n",
    "    req = urllib.request.Request(url , headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    page = urllib.request.urlopen(req).read()\n",
    "    soup_data = BeautifulSoup(page, \"html.parser\")\n",
    "    return soup_data\n",
    "\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(url):\n",
    "    soup = make_soup(url)\n",
    "    if soup == -1:\n",
    "        return ''\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "\n",
    "def get_words(url):\n",
    "    text = text_from_html(url)\n",
    "    # Delete punctuaction and empty words\n",
    "    text = [re.sub('\\W+', '', t) for t in text.lower().split()]\n",
    "    text = list(filter(lambda t: len(t), text))\n",
    "    return text\n",
    "\n",
    "def get_hrefs(url):\n",
    "    soup = make_soup(url)\n",
    "    if soup == -1:\n",
    "        return []\n",
    "    hrefs = []\n",
    "        \n",
    "    for link in soup.find_all('a'):\n",
    "        hrefs.append(link.get('href'))\n",
    "        \n",
    "    ans = []\n",
    "    for link in hrefs:\n",
    "        if link is None or len(link) == 0:\n",
    "            continue\n",
    "        if link.find('https') != -1 or link.find('http') != -1:\n",
    "            ans.append(link)\n",
    "        else:\n",
    "            sub_sites = link.split('/')\n",
    "            if len(sub_sites) > 1:\n",
    "                key_word = '/' + sub_sites[1] + '/'\n",
    "                pos = url.find(key_word)\n",
    "                if pos != -1:\n",
    "                    ans.append(url[:pos] + link)\n",
    "                else:\n",
    "                    ans.append(url + link)\n",
    "          \n",
    "    return ans\n",
    "\n",
    "def get_dict():\n",
    "    return dict(sorted(WORDS.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "\n",
    "def BFS(url, max_steps):\n",
    "    Q = deque()\n",
    "    Q.append((url, 0))\n",
    "    visited = set()\n",
    "    visited.add(url)\n",
    "    \n",
    "    while len(Q):\n",
    "        act_url = Q[0][0]\n",
    "        act_steps = Q[0][1]\n",
    "        Q.popleft()\n",
    "    \n",
    "        print(f'Remaining: {len(Q)}. Act: ', act_url)\n",
    "        \n",
    "        words = get_words(act_url)\n",
    "        hrefs = get_hrefs(act_url)\n",
    "    \n",
    "        for w in words:\n",
    "            WORDS.setdefault(w, set()).add(act_url)\n",
    "\n",
    "        for no, ref in enumerate(hrefs):\n",
    "            if act_steps < max_steps and not ref in visited:\n",
    "                Q.append((ref, act_steps + 1))\n",
    "                visited.add(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(url, depth):\n",
    "    WORDS.clear()\n",
    "    BFS(url, depth)\n",
    "    return get_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a site\n",
    "site = 'https://en.wikipedia.org/wiki/Special:Random'\n",
    "site2 = 'https://en.wikipedia.org/wiki/Mercury_Cyclone'\n",
    "site3 = 'https://en.wikipedia.org/wiki/Alpman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining: 0. Act:  https://en.wikipedia.org/wiki/Alpman\n",
      "Remaining: 55. Act:  https://en.wiktionary.org/wiki/Alpman\n",
      "Remaining: 54. Act:  https://en.wikipedia.org/wiki/Ayten_Alpman\n",
      "Remaining: 53. Act:  https://en.wikipedia.org/wiki/Fatma_Serpil_Alpman\n",
      "Remaining: 52. Act:  https://en.wikipedia.org/wiki/Surname\n",
      "Remaining: 51. Act:  https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Alpman&namespace=0\n",
      "Remaining: 50. Act:  https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking\n",
      "Remaining: 49. Act:  https://en.wikipedia.org/wiki/Given_name\n",
      "Remaining: 48. Act:  https://en.wikipedia.org/w/index.php?title=Alpman&oldid=925236056\n",
      "Remaining: 47. Act:  https://en.wikipedia.org/wiki/Help:Category\n",
      "Remaining: 46. Act:  https://en.wikipedia.org/wiki/Category:Surnames\n",
      "Remaining: 45. Act:  https://en.wikipedia.org/wiki/Category:Articles_with_short_description\n",
      "Remaining: 44. Act:  https://en.wikipedia.org/wiki/Category:All_set_index_articles\n",
      "Remaining: 43. Act:  https://en.wikipedia.org/wiki/Category:Monitored_short_pages\n",
      "Remaining: 42. Act:  https://en.wikipedia.org/wiki/Special:MyTalk\n",
      "Remaining: 41. Act:  https://en.wikipedia.org/wiki/Special:MyContributions\n",
      "Remaining: 40. Act:  https://en.wikipedia.org/wiki/Alpman/w/index.php?title=Special:CreateAccount&returnto=Alpman\n",
      "Ignored:  HTTP Error 404: Not Found\n",
      "Ignored:  HTTP Error 404: Not Found\n",
      "Remaining: 39. Act:  https://en.wikipedia.org/wiki/Alpman/w/index.php?title=Special:UserLogin&returnto=Alpman\n",
      "Ignored:  HTTP Error 404: Not Found\n",
      "Ignored:  HTTP Error 404: Not Found\n",
      "Remaining: 38. Act:  https://en.wikipedia.org/wiki/Talk:Alpman\n",
      "Remaining: 37. Act:  https://en.wikipedia.org/wiki/Alpman/w/index.php?title=Alpman&action=edit\n",
      "Remaining: 36. Act:  https://en.wikipedia.org/wiki/Alpman/w/index.php?title=Alpman&action=history\n",
      "Ignored:  HTTP Error 404: Not Found\n",
      "Ignored:  HTTP Error 404: Not Found\n",
      "Remaining: 35. Act:  https://en.wikipedia.org/wiki/Main_Page\n",
      "Remaining: 34. Act:  https://en.wikipedia.org/wiki/Wikipedia:Contents\n",
      "Remaining: 33. Act:  https://en.wikipedia.org/wiki/Portal:Featured_content\n",
      "Remaining: 32. Act:  https://en.wikipedia.org/wiki/Portal:Current_events\n",
      "Remaining: 31. Act:  https://en.wikipedia.org/wiki/Special:Random\n",
      "Remaining: 30. Act:  https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\n",
      "Remaining: 29. Act:  https://shop.wikimedia.org\n",
      "Remaining: 28. Act:  https://en.wikipedia.org/wiki/Help:Contents\n",
      "Remaining: 27. Act:  https://en.wikipedia.org/wiki/Wikipedia:About\n",
      "Remaining: 26. Act:  https://en.wikipedia.org/wiki/Wikipedia:Community_portal\n",
      "Remaining: 25. Act:  https://en.wikipedia.org/wiki/Special:RecentChanges\n",
      "Remaining: 24. Act:  https://en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "Remaining: 23. Act:  https://en.wikipedia.org/wiki/Special:WhatLinksHere/Alpman\n",
      "Remaining: 22. Act:  https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Alpman\n",
      "Remaining: 21. Act:  https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard\n",
      "Remaining: 20. Act:  https://en.wikipedia.org/wiki/Special:SpecialPages\n",
      "Remaining: 19. Act:  https://en.wikipedia.org/wiki/Alpman/w/index.php?title=Alpman&oldid=925236056\n",
      "Remaining: 18. Act:  https://en.wikipedia.org/wiki/Alpman/w/index.php?title=Alpman&action=info\n",
      "Remaining: 17. Act:  https://www.wikidata.org/wiki/Special:EntityPage/Q56537475\n",
      "Remaining: 16. Act:  https://en.wikipedia.org/wiki/Alpman/w/index.php?title=Special:CiteThisPage&page=Alpman&id=925236056\n",
      "Ignored:  HTTP Error 404: Not Found\n",
      "Ignored:  HTTP Error 404: Not Found\n",
      "Remaining: 15. Act:  https://en.wikipedia.org/wiki/Alpman/w/index.php?title=Special:Book&bookcmd=book_creator&referer=Alpman\n",
      "Ignored:  HTTP Error 404: Not Found\n",
      "Ignored:  HTTP Error 404: Not Found\n",
      "Remaining: 14. Act:  https://en.wikipedia.org/wiki/Alpman/w/index.php?title=Special:ElectronPdf&page=Alpman&action=show-download-screen\n",
      "Ignored:  HTTP Error 404: Not Found\n",
      "Ignored:  HTTP Error 404: Not Found\n",
      "Remaining: 13. Act:  https://en.wikipedia.org/wiki/Alpman/w/index.php?title=Alpman&printable=yes\n",
      "Ignored:  HTTP Error 404: Not Found\n",
      "Ignored:  HTTP Error 404: Not Found\n",
      "Remaining: 12. Act:  https://www.wikidata.org/wiki/Special:EntityPage/Q56537475#sitelinks-wikipedia\n",
      "Remaining: 11. Act:  https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\n",
      "Remaining: 10. Act:  https://creativecommons.org/licenses/by-sa/3.0/\n",
      "Ignored:  HTTP Error 403: Forbidden\n",
      "Ignored:  HTTP Error 403: Forbidden\n",
      "Remaining: 9. Act:  https://foundation.wikimedia.org/wiki/Terms_of_Use\n",
      "Remaining: 8. Act:  https://foundation.wikimedia.org/wiki/Privacy_policy\n",
      "Remaining: 7. Act:  https://www.wikimediafoundation.org/\n",
      "Remaining: 6. Act:  https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer\n",
      "Remaining: 5. Act:  https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\n",
      "Remaining: 4. Act:  https://stats.wikimedia.org/v2/#/en.wikipedia.org\n",
      "Remaining: 3. Act:  https://foundation.wikimedia.org/wiki/Cookie_statement\n",
      "Remaining: 2. Act:  https://en.m.wikipedia.org/w/index.php?title=Alpman&mobileaction=toggle_view_mobile\n",
      "Remaining: 1. Act:  https://wikimediafoundation.org/\n",
      "Remaining: 0. Act:  https://www.mediawiki.org/\n",
      "Wall time: 53.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's see all pages available from starting one \n",
    "words = scrap(site3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total collected 11848 words\n",
      "Most common word is 'from' and occured on 48 websites!\n"
     ]
    }
   ],
   "source": [
    "print(f'In total collected {len(words)} words')\n",
    "\n",
    "pair = next(iter(words.items()))\n",
    "print(f'Most common word is \\'{pair[0]}\\' and occured on {len(pair[1])} websites!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Speed up by using multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_dict():\n",
    "    return dict(sorted(WORDS.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "def BFS_multi(url, max_steps):\n",
    "    cores = multiprocessing.cpu_count() - 1\n",
    "    cores = 100\n",
    "    sites = [url]\n",
    "\n",
    "    for no in range(max_steps):\n",
    "        act_sites = []\n",
    "        print(f'Step {no} out of {max_steps}')\n",
    "        for i in range(0, len(sites), cores):\n",
    "            print(f'Looking for sites, batch {i // cores} out of {len(sites) // cores}')\n",
    "            batch = sites[i: min(i + cores, len(sites))]\n",
    "            p = Pool(cores)\n",
    "            next_sites = p.map(get_hrefs, batch)\n",
    "            p.terminate()\n",
    "            p.join()\n",
    "            for p_site in next_sites:\n",
    "                act_sites += p_site\n",
    "\n",
    "        sites = act_sites\n",
    "\n",
    "    words = []\n",
    "    for i in range(0, len(sites), cores):\n",
    "        print(f'Scrapping words, batch {i // cores} out of {len(sites) // cores}')\n",
    "        batch = sites[i: min(i + cores, len(sites))]\n",
    "        p = Pool(len(sites))\n",
    "        act_words = p.map(get_words, batch)\n",
    "        words += act_words\n",
    "\n",
    "\n",
    "    for i, word_list in enumerate(words):\n",
    "        print(f'Counting words from site: {i} / {len(words)}')\n",
    "        for w in word_list:\n",
    "            WORDS[w] = WORDS.get(w, 0) + 1\n",
    "\n",
    "\n",
    "    print('\\n\\n---SITES---\\n\\n')\n",
    "    for i, s in enumerate(sites):\n",
    "        print(f'{i} / {len(sites)} -> : ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_scrap(url, depth):\n",
    "    WORDS.clear()\n",
    "    BFS_multi(url, depth)\n",
    "    return get_multi_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 out of 1\n",
      "Looking for sites, batch 0 out of 0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ ==  '__main__':\n",
    "    #doesn't work on jupyter\n",
    "    words = multi_scrap(site3, depth=1)\n",
    "    print(f'In total collected {len(words)} words')\n",
    "\n",
    "    pair = next(iter(words.items()))\n",
    "    print(f'Most common word is \\'{pair[0]}\\' and occured {pair[1]} times!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
